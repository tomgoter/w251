# W251 - Summer 2020
## Homework 5 - TF2 and TF1
### Tom Goter

## Introduction to Tensorflow v2
In this portion of the homework, we were asked to go through two Jupyter Notebooks in a Docker Container on our Jetson device. The first lab was a straightforward introduction to using a Sequential Model in Keras to perform classification on the MNIST data. The provided example in the notebook used a fully connected layer with 128 hidden units.  We were asked to determine whether we could improve the model accuracy. To this end I constructed two challenger models. The first model was simply a two-layer version of the first model with increased regularization to prevent overfitting. This model was able to slightly improve accuracy to the tune of 0.2% (i.e., 97.8% vs 97.6%). The second challenger model was a two-layer convolutional neural network with heavy regularization in the form of dropout after the pooling layers and prior to the classification layer.This model showed a significant increase in performance - achieving a test set accuracy of 98.9%.

The second notebook in the first part of the homework dealt with transfer learning. We were asked to load in a pretraing Mobilent feature extraction model (i.e., the mobile net model pretrained on imaged net with everything but the classification layer). This allows us to put any sort of additional layer(s) on top of all of the convolution layers which have been pre-trained to recognize certain components of images. In our case we replace the 1000 node classification layer that would have been used during ImageNet training with a five node classification layer as we are using the pretrained model to enhance and speed up our ability to classify images of flowers. In order to avoid out-of-memory errors on the Jetson, we use a batch size of 16 instead of 32. We investigate using frozen layers and unfrozen layers in our transfered feature extraction model. In general, we see that either way we see immediate and substantial increases in accuracy during our training which shows us the benefit of the transfer learning.

Notebook files and HTML versions of the completed notebooks are available in this repository as:  
1. beginner.ipynb - Notebook for MNIST classification  
2. beginner.HTML - HTML version of completed MNIST classification notebook with challenger models  
3. transfer_learning_with_hub.ipynb - Notebook for transfer learning with mobilenet  
4. transfer_learning_with_hub.html - HTML version of completed mobilenet transfer learning notebook  

## Comparison to Tensorflow v1
1. **What is TensorFlow? Which Company is the leading contributor to TensorFlow**? - TensorFlow is a high performance software library for performing numerical computations. Essentially it is optimized for analysis of multi-dimensional arrays (or tensors). It was originally developed by Google. It uses graph based exeution to optimize computations.
2. **What is TensorRT? How is it different from TensorFlow?**  - TensorRT is a software development kit provided by NVIDIA that allows someone the ability to optimize their trained model for inference on particular hardware (e.g., NVIDIA GPUs of a particlar architecture). TensorRT also provides quantization support for reduced model sizes as well. This software package can greatly increase the throughput of inference on your edge device. So TensorRT is really a back-end optimization for putting a deep learning solution into production in a low latency manner.  
3. **What is ImageNet? How many images does it contain? How many classes?** - ImageNet is the term generally used to refer to a set of 1.2 million images that were part of the ImageNet Large Scale Visual Recognition Challenge. This is actually a subset of the overall ImageNet data set. There is a more complete version of this dataset that contains 14M images with labels from 21 thousand distinct classes. The smaller subset used when AlexNet changed the SoTA and showed us the power of deep learning had only 1000 classes.  
4. **What is the difference between MobileNet and GoogLeNet?** - MobileNet is a fairly straightforward architecture.( We discuss v2 here.) It consists of stacked modules that are identical. Basically each module consists of an expansion in which a set of features is expanded in the filter dimensing a 1x1 convolution layer. A 3x3 kernel with no depth reduction or expansion is then used, followed by a neck down to the starting filter depth using the 1x1 convolution layer again. There is also a skip or residual connection from the beginning of the module to the end of the module. For GoogleNet or InceptionNetv1 - a similar process is used but instead of just using 3x3 kernels, 5x5 kernels are also used, as well as a pooling layer. At the end of the layer the outputs of each of these pieces is concatenated. So MobileNet is slightly simpler in my estimation, as is its point. It has only 3.47M parameters; whereas, GoogLeNet has about 6.8M trainable parameters.  
5. **In your own words, what is a bottleneck?** A bottleneck is the portion of a deep learning model in which the number of parameters is greatly reduced. In many networks this happens in the layer prior to the classification layer. But a bottleneck can also be implemented as part of a building block or module as it is in the last layer of the MobileNet modules. 
6. **How is a bottleneck different from the concept of layer freezing?** - A bottleneck is just a way to reduce the dimensionality of the network. Layer freezing is kind of a different concept altogether. Layer freezing is simply not allowing weights to be updated for any layer in your model. You may want to do this when transfer learning to reduce the amount of time it takes to finetune your model. A good place to freeze your model would be at th bottleneck because prior to the bottleneck the representatio of the network would be much larger.  
7. **In the TF1 lab, you trained the last layer (all the previous layers retain their already-trained state). Explain how the lab used the previous layers (where did they come from? how were they used in the process?)** - When loading the pretrained model, the retrained model starts going through the images in randomly generated batches. Each time it sees a new image, it caches the output of the network at the bottleneck stage into a bottleneck file. Since we loop over the images multiple time (number of epochs) during training, and because our weights are frozen, we would be wasting our time if we were to re-calculate the network output for these repeat images each time.
