# W251 - Summer 2020
## Homework 5 - TF2 and TF1
### Tom Goter

## Introduction to Tensorflow v2
In this portion of the homework, we were asked to go through two Jupyter Notebooks in a Docker Container on our Jetson device. The first lab was a straightforward introduction to using a Sequential Model in Keras to perform classification on the MNIST data. The provided example in the notebook used a fully connected layer with 128 hidden units.  We were asked to determine whether we could improve the model accuracy. To this end I constructed two challenger models. The first model was simply a two-layer version of the first model with increased regularization to prevent overfitting. This model was able to slightly improve accuracy to the tune of 0.2% (i.e., 97.8% vs 97.6%). The second challenger model was a two-layer convolutional neural network with heavy regularization in the form of dropout after the pooling layers and prior to the classification layer.This model showed a significant increase in performance - achieving a test set accuracy of 98.9%.

The second notebook in the first part of the homework dealt with transfer learning. We were asked to load in a pretraing Mobilent feature extraction model (i.e., the mobile net model pretrained on imaged net with everything but the classification layer). This allows us to put any sort of additional layer(s) on top of all of the convolution layers which have been pre-trained to recognize certain components of images. In our case we replace the 1000 node classification layer that would have been used during ImageNet training with a five node classification layer as we are using the pretrained model to enhance and speed up our ability to classify images of flowers. In order to avoid out-of-memory errors on the Jetson, we use a batch size of 16 instead of 32. We investigate using frozen layers and unfrozen layers in our transfered feature extraction model. In general, we see that either way we see immediate and substantial increases in accuracy during our training which shows us the benefit of the transfer learning.

Notebook files and HTML versions of the completed notebooks are available in this repository as:  
1. beginner.ipynb - Notebook for MNIST classification  
2. beginner.HTML - HTML version of completed MNIST classification notebook with challenger models  
3. transfer_learning_with_hub.ipynb - Notebook for transfer learning with mobilenet  
4. transfer_learning_with_hub.html - HTML version of completed mobilenet transfer learning notebook  

## Comparison to Tensorflow v1
1. **What is TensorFlow? Which Company is the leading contributor to TensorFlow**? - TensorFlow is a high performance software library for performing numerical computations. Essentially it is optimized for analysis of multi-dimensional arrays (or tensors). It was originally developed by Google. It uses graph based exeution to optimize computations.
2. **What is TensorRT? How is it different from TensorFlow?**  - TensorRT is a software development kit provided by NVIDIA that allows someone the ability to optimize their trained model for inference on particular hardware (e.g., NVIDIA GPUs of a particlar architecture). TensorRT also provides quantization support for reduced model sizes as well. This software package can greatly increase the throughput of inference on your edge device. So TensorRT is really a back-end optimization for putting a deep learning solution into production in a low latency manner.  
3. **What is ImageNet? How many images does it contain? How many classes?** - ImageNet is the term generally used to refer to a set of 1.2 million images that were part of the ImageNet Large Scale Visual Recognition Challenge. This is actually a subset of the overall ImageNet data set. There is a more complete version of this dataset that contains 14M images with labels from 21 thousand distinct classes. The smaller subset used when AlexNet changed the SoTA and showed us the power of deep learning had only 1000 classes.  
4. **What is the difference between MobileNet and GoogLeNet?** - MobileNet is a fairly straightforward architecture.( We discuss v2 here.) It consists of stacked modules that are identical. Basically each module consists of an expansion in which a set of features is expanded in the filter dimensing a 1x1 convolution layer. A 3x3 kernel with no depth reduction or expansion is then used, followed by a neck down to the starting filter depth using the 1x1 convolution layer again. There is also a skip or residual connection from the beginning of the module to the end of the module. For GoogleNet or InceptionNetv1 - a similar process is used but instead of just using 3x3 kernels, 5x5 kernels are also used, as well as a pooling layer. At the end of the layer the outputs of each of these pieces is concatenated. So MobileNet is slightly simpler in my estimation, as is its point. It has only 3.47M parameters; whereas, GoogLeNet has about 6.8M trainable parameters.  
5. **In your own words, what is a bottleneck?** A bottleneck is the portion of a deep learning model in which the number of parameters is greatly reduced. In many networks this happens in the layer prior to the classification layer. But a bottleneck can also be implemented as part of a building block or module as it is in the last layer of the MobileNet modules. 
6. **How is a bottleneck different from the concept of layer freezing?** - A bottleneck is just a way to reduce the dimensionality of the network. Layer freezing is kind of a different concept altogether. Layer freezing is simply not allowing weights to be updated for any layer in your model. You may want to do this when transfer learning to reduce the amount of time it takes to finetune your model. A good place to freeze your model would be at th bottleneck because prior to the bottleneck the representatio of the network would be much larger.  
7. **In the TF1 lab, you trained the last layer (all the previous layers retain their already-trained state). Explain how the lab used the previous layers (where did they come from? how were they used in the process?)** - When loading the pretrained model, the retrained model starts going through the images in randomly generated batches. Each time it sees a new image, it caches the output of the network at the bottleneck stage into a bottleneck file. Since we loop over the images multiple time (number of epochs) during training, and because our weights are frozen, we would be wasting our time if we were to re-calculate the network output for these repeat images each time.  
8. **How does a low --learning_rate (step 7 of TF1) value (like 0.005) affect the precision? How much longer does training take?** - Validation accuracy was increased by about 2%. Training time per step was the same, but it took about 200 more steps to find validation accuracy equlilibrium, as expected.  
9. **How about a --learning_rate (step 7 of TF1) of 1.0? Is the precision still good enough to produce a usable graph?** - As you would expect with a large learning rate, the training and validation errors tended to oscillate a bit. The validation accuracy still ended up around 0.88 on-average though. The time to train 2000 steps was still the same which also makes sense. Relative steady-state accuracies were reached within 400 steps (comparable to baseline).
10. **Use your own . How accurate was your model? Were you able to train it using a few images, or did you need a lot?** - I used a very small set of 12 images each from three different classes. These classes were all different dog breeds: Pomsky, Wheaten Terrier and Labrador Retrievers. I used nine images for validaiton and 27 for training with a batch size of three. With such a small dataset, the network was easily able to memorize these photos and within 50 steps validation accuracy was 100%.
11. **Run the TF1 script on the CPU (see instructions above) How does the training time compare to the default network training (section 4)? Why?** The training time is about the same as when using the GPU. I would imagine that this is due to the use of the bottlenecks file. Since we are only actually training the classification layer, there isn't enough work to do for the GPU to be particularly beneficial.
12. **Try the training again, but this time do export ARCHITECTURE="inception_v3" Are CPU and GPU training times different?** - When training with Inception net on the GPU, it took about twice as long as MobileNet.
13. **Given the hints under the notes section, if we trained Inception_v3, what do we need to pass to replace ??? below to the label_image script? Can we also glean the answer from examining TensorBoard?** Use the following line to run the label_image script `python3 -m scripts.label_image --input_layer=Mul --input_height=229 --input_width=229  --graph=tf_files/retrained_graph.pb --image=tf_files/flower_photos/daisy/21652746_cc379e0eea_m.jpg`
